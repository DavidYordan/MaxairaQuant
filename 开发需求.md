# MaxairaQuant 量化交易系统开发需求文档

## 项目概述

MaxairaQuant 是一个基于 Java + Python 混合技术栈的量化交易系统，专注于加密货币市场的实时数据采集、策略回测和自动化交易。系统采用模块化设计，确保高性能、高可靠性和易扩展性。

## 系统架构概览

整个系统按照算法交易系统的常见架构划分为若干模块：

1. **数据采集模块**：实时行情数据采集和存储
2. **数据补齐模块**：历史数据补齐和数据完整性保证
3. **策略回测模块**：历史数据回测和策略验证
4. **实盘交易模块**：实时交易执行和风险控制
5. **数据存储模块**：高性能时序数据库存储
6. **前端可视化模块**：图形界面展示和监控

各模块既相互独立又协同工作，通过数据库或消息机制交互，方便故障隔离与扩展升级。

## 技术选型与模块划分

### 编程语言选择
- **Java 21 LTS**：用于长期运行且稳定性要求高的服务（数据采集、交易执行）
- **Python 3.11+**：用于开发效率要求高、运算密集的任务（策略回测、数据分析）
- **Vue3 + TypeScript**：前端可视化界面

### 数据库选择
- **ClickHouse**：主要时序数据存储，支持高频写入和快速查询
- **Redis**：缓存和消息队列（可选）

## 已完成功能模块

### ✅ 1. 实时数据采集服务 (Market Data Service)

**技术实现**：Java 21 + Spring Boot 3.2

**核心类**：
- `WebSocketService`：币安 WebSocket 连接管理
- `DataBufferService`：数据缓冲和批量写入
- `DatabaseService`：ClickHouse 数据库操作
- `AggTrade`：交易数据模型

**功能特性**：
- ✅ WebSocket 实时订阅 ETH/USDT 聚合交易流
- ✅ 自动重连和心跳检测机制
- ✅ 生产者-消费者模式的数据处理
- ✅ 批量插入策略（10000条/5秒触发）
- ✅ 完善的错误处理和日志记录
- ✅ Spring Actuator 健康检查接口

### ✅ 2. 历史数据补齐服务 (Data Backfill Service)

**技术实现**：Java 21 + Spring Boot 3.2

**核心类**：
- `DataBackfillService`：数据补齐调度服务
- `DataGapDetector`：数据缺口检测器
- `BinanceDataDownloader`：币安数据下载器
- `SyncStatusService`：同步状态管理
- `DataSyncStatus`：同步状态数据模型

**智能分层补齐策略**：
- ✅ **月级补齐**：下载币安官方月度数据包（.zip格式）
- ✅ **日级补齐**：下载对应日期的日度数据包
- ✅ **小时级补齐**：通过 REST API 补齐小时级缺口
- ✅ **实时补齐**：WebSocket 重连后补齐断线期间数据

**功能特性**：
- ✅ 基于状态表的增量检测机制
- ✅ 智能跳跃已完成的时间段
- ✅ 并发处理不同类型的缺口
- ✅ 幂等性保证和断点续传
- ✅ 指数退避重试策略
- ✅ 定时调度（每小时执行一次）

### ✅ 3. 数据存储设计

**数据库表结构**：
- ✅ `ethusdt`：原始交易数据表
- ✅ `data_sync_status`：数据同步状态表
- ✅ `ethusdt_kline_*`：多周期K线数据表（5s/1m/15m/1h）

**物化视图**：
- ✅ 自动生成多周期K线数据
- ✅ 层级聚合：原始数据→5秒→1分钟→15分钟→1小时

### ✅ 4. 配置管理

**配置文件**：
- ✅ `application.yml`：Spring Boot 配置
- ✅ 多环境配置支持（dev/prod）
- ✅ ClickHouse 连接配置
- ✅ 币安 API 配置
- ✅ 调度任务配置

## 待完成功能模块

### 🔄 5. 策略回测引擎 (Backtest Engine)

**技术实现**：Python 3.11+ + pandas/numpy

**计划功能**：
- 历史数据回测框架
- 多种技术指标计算
- 策略性能评估
- 回测结果可视化
- 参数优化功能

### 🔄 6. 实盘交易引擎 (Live Trading Engine)

**技术实现**：Java 21 + Spring Boot 3.2

**计划功能**：
- 实时策略信号处理
- 订单管理和执行
- 风险控制模块
- 仓位管理
- 交易记录和监控

### 🔄 7. 前端可视化 (Frontend Dashboard)

**技术实现**：Vue3 + TypeScript + ECharts

**计划功能**：
- 实时行情图表展示
- K线图和技术指标
- 回测结果可视化
- 交易监控面板
- 系统状态监控

---

## 原始需求文档

深度完善的开发需求文档
系统架构概览

整个系统将按照算法交易系统的常见架构划分为若干模块，包括数据采集、数据补齐/处理、策略与回测、实盘交易执行、数据存储以及前端可视化等
blog.csdn.net
。各模块既相互独立又协同工作，确保系统高效、稳定，并便于未来扩展。整体流程如下：

市场数据接入：通过交易所提供的 WebSocket 获取实时行情（约100ms一帧），确保最低延迟；必要时通过REST API获取历史数据用于回补和校验
blog.csdn.net
。

数据存储与处理：采用高性能时序数据库（如 ClickHouse）保存行情和衍生的K线数据。利用其高并发写入和压缩能力支撑实时数据摄取和历史数据查询
clickhouse.com
clickhouse.com
。

策略模块：基于实时/历史行情计算交易信号，可使用技术指标或机器学习算法等。策略模块既用于离线回测也用于实盘信号生成，但运行环境相互隔离。

交易执行模块：接收策略信号并通过交易所API执行下单、撤单等操作，包含基础风控规则，确保交易指令安全下达
blog.csdn.net
。

可视化前端：提供图形界面展示行情K线、回测结果和实盘交易状况，实现对股东/合伙人友好的可视化交互。

上述各模块初期部署在同一台服务器上，通过不同进程（screen会话）运行。模块间通过数据库或消息机制交互，方便故障隔离与扩展升级。未来若需提升性能或可靠性，可以将各模块部署到分布式环境甚至云端，实现水平扩展。

技术选型与模块划分

编程语言方面，将根据模块需求选取合适的语言实现：Python 用于开发效率要求高、运行周期短的任务，Java 用于长期运行且稳定性要求高的服务
blog.csdn.net
。这种“Python + Java”混合技术栈在量化交易系统中十分常见，Python 和 Java 各自发挥所长共同构成高性能生产环境
quantstart.com
。

实时数据采集服务（Market Data Service）：使用 Java 21 LTS 实现。Java 在多线程和长期稳定运行方面具有优势，特别适合作为持续运行的行情采集进程。该服务通过币安WebSocket订阅 ETH/USDT 聚合交易流，实时接收数据并批量写入ClickHouse数据库。

技术实现要点：
- 使用Spring Boot 3.2框架提供依赖注入和配置管理
- 采用OkHttp WebSocket客户端，支持自动重连和心跳检测
- 实现生产者-消费者模式：WebSocket接收数据放入内存队列，独立线程批量消费写入数据库
- 批量插入策略：每10000条记录或5秒间隔触发一次批量写入，平衡实时性和性能
- 完善的错误处理：网络断线自动重连，数据库异常重试，异常情况报警
- 健康检查接口：通过Spring Actuator暴露服务状态，便于监控

历史数据补齐程序（Data Backfill Service）：使用 Java 21 实现，保持技术栈统一。该程序实现智能分层补齐机制，根据数据缺失的时间跨度选择最优的补齐策略，避免全量扫描，提高补齐效率。

## 智能分层补齐策略

### 1. 补齐时间粒度分层
- **月级补齐**：从2024年1月1日起，如果缺少整个月的数据，直接下载币安官方月度数据包（.zip格式）
- **日级补齐**：如果缺少整天的数据，下载对应日期的日度数据包
- **小时级补齐**：对于当日数据，检测小时级别的缺口，通过REST API补齐
- **实时补齐**：WebSocket断线重连后，补齐断线期间的数据缺口

### 2. 辅助状态表设计
创建 `data_sync_status` 表记录补齐进度：
```sql
CREATE TABLE data_sync_status (
    symbol LowCardinality(String),
    sync_type Enum8('monthly'=1, 'daily'=2, 'hourly'=3, 'realtime'=4),
    period_start DateTime64(3),
    period_end DateTime64(3),
    status Enum8('pending'=1, 'processing'=2, 'completed'=3, 'failed'=4),
    last_updated DateTime64(3) DEFAULT now64(),
    retry_count UInt8 DEFAULT 0,
    error_message String DEFAULT ''
) ENGINE = MergeTree()
ORDER BY (symbol, sync_type, period_start);
```

### 3. 增量检测机制
- **启动时检测**：服务启动时，从状态表读取上次处理位置，仅检测增量时间段
- **定时检测**：每小时检测一次数据完整性，重点关注最近24小时的数据
- **智能跳跃**：已标记为 `completed` 的时间段直接跳过，不重复检查

### 4. 技术实现要点
- **分层下载器**：`MonthlyDataDownloader`、`DailyDataDownloader`、`HourlyDataDownloader`
- **缺口检测器**：`DataGapDetector` 基于状态表和实际数据进行智能检测
- **进度管理器**：`SyncProgressManager` 管理各层级的同步状态
- **幂等性保证**：通过唯一键约束和状态检查，确保重复运行不产生重复数据
- **断点续传**：支持从任意中断点恢复，基于状态表记录的进度信息
- **并发控制**：不同时间段可并行下载，但同一时间段避免重复处理

### 5. 补齐优先级策略
1. **历史数据**：优先补齐月度→日度→小时级数据
2. **近期数据**：重点关注最近7天的数据完整性
3. **实时数据**：WebSocket重连后立即补齐断线期间的缺口
4. **错误重试**：失败的补齐任务按指数退避策略重试

服务拆分：数据采集和补齐作为两个独立的Spring Boot应用运行。实时采集服务专注高频行情写入，补齐程序作为后台任务定时运行，避免二者互相影响。两个服务可以部署在同一台服务器的不同端口，通过systemd管理服务生命周期。

策略回测引擎（Backtest Engine）：使用 Python 实现。回测涉及大量历史数据计算，Python拥有NumPy/Pandas等高效数值库，能够方便地进行指标计算和结果分析
quantstart.com
。开发者也可借助现有框架（如 Backtrader）加速策略开发
blog.csdn.net
。Python的快速迭代特性让策略逻辑的验证和调整更加高效。同时，回测模块不需要长期不间断运行，Python的运行性能可以接受。

实盘交易引擎（Live Trading Engine）：建议使用 Java 实现，并与其它模块物理隔离运行。实盘交易对稳定性要求最高，任何异常都可能产生经济损失。采用Java可利用其强类型和JVM稳定性的优势，降低运行时错误。同时将交易引擎独立为单独进程（与行情、策略隔离），即使其它模块崩溃也不影响交易进程，保障实盘交易的连续性和安全性。Java在执行性能上也优于纯Python，有助于在100ms节奏下低延迟地下单。
(注：在非毫秒级的策略下，Python完全可以胜任交易执行，但考虑长期可靠性和未来更高频需求，Java会是更稳妥的选择
blog.csdn.net
。)

前端可视化：采用 Vue3 前端框架构建单页应用。Vue3能够高效地构建交互界面，并且生态中有丰富的图表组件用于金融数据展示。前端通过HTTP API或WebSocket从后端获取数据，实现行情和策略结果的可视化（后续详述）。

模块通信与扩展：目前单机部署下，各服务可以通过数据库共享数据或使用本地网络通信。随着需求提升，可考虑引入消息队列/事件总线等机制，让行情服务将数据广播给策略和交易模块，减少重复订阅和数据库轮询。例如，可使用Redis发布订阅或Kafka等，在行情采集到数据时同时分发给交易引擎，实现更实时的解耦通信。初期系统规模小，可暂时不引入复杂消息系统，但设计上应预留接口，方便未来升级为分布式部署
blog.quantinsti.com
blog.quantinsti.com
。

数据存储设计 (ClickHouse 时序数据库)

数据存储选用 ClickHouse 列式数据库，以满足海量行情数据的存储和快速查询需求。ClickHouse擅长高频写入和基于时间的分析查询，内置的压缩和分区机制非常适合行情/订单这类追加型时序数据
clickhouse.com
。针对本项目，数据库设计考虑以下要点：

库表规划：为每种交易对建立独立的数据表，或者建立一张通用的行情表包含交易对字段，两种方案各有利弊。用户建议不使用symbol字段、直接按交易对建表，理由是表名已体现交易对，可节省存储空间。确实，若只有ethusdt一个表，则其中数据天然都是ETH/USDT，不需要额外字段区分。将来扩展BTC/USDT时，再新建btcusdt表即可，每张表只存单一品种的数据。这样可以避免在每行重复存储交易对标识，从而节省一些空间。

然而需要说明，ClickHouse 对重复字符串存储的开销通过LowCardinality类型和压缩已大大降低。即使将交易对作为一列，也可用LowCardinality(String)存储，仅以整数ID替代原始字符串，在唯一值不多的情况下“显著减少存储开销”
clickhouse.com
。同时，列式压缩对重复值效果很好，即便单表含多交易对也不会占用太多额外空间
clickhouse.com
。因此统一行情表+symbol字段也是可行方案，其优点是无需频繁增删表，查询不同交易对的历史数据也更灵活。
权衡：考虑到项目初期只涉及少数主流币（ETH、BTC等），采用分表方案问题不大，结构清晰且每表数据量可控。同时节省下symbol列也有一点点好处。后期如果币种很多，可能需要切换为“大表+symbol列”模式以方便管理。在当前范围内，我们建议沿用每个币种建一张表的设计，但在表结构中预留扩展性。例如可以封装建表语句，未来需要时批量生成新表。

基础行情表结构：无论采用哪种表方案，每张行情明细表需要存储原始市场数据（可以是逐笔成交、盘口快照等）。假设我们采集的是逐笔成交Tick数据，表字段可设计为：timestamp (精确到ms或更高), price, volume 等，可能还包括交易所提供的其它字段（如交易ID等）。如果采集的是简化的报价（每100ms的最新价和成交量），字段相应调整。选择合适的MergeTree引擎，主键包含时间（例如按timestamp排序）以便快速按时间范围查询。对于多表方案，表名区分品种，无需symbol列；如果单表方案，则加一个symbol LowCardinality(String)字段作为主键的一部分（例如排序键ORDER BY (symbol, timestamp))。

辅助状态表：为了优化数据补齐和批处理流程，我们将设计一张辅助表（例如sync_status或progress_tracker），记录各数据表的处理进度。该表包含每个交易对最新完整处理到的时间戳或ID。例如，记录ethusdt已补齐到哪个日期/ID，btcusdt已处理到哪里。补齐程序每次启动时，先从该表读取上次处理位置，从而仅增量处理新增的数据，无需每次都从头扫描全量历史
medium.com
。这种“检查点”机制是常见的增量数据同步策略，可避免重复读取大量已处理数据
medium.com
。在ClickHouse中，由于获取最大时间戳或ID可以通过主键索引完成，开销不大，但明确记录状态可以简化逻辑、提高可靠性。

数据清理和归档：随着时间推移，行情明细数据会非常庞大。ClickHouse支持按时间分区清理旧数据或将其归档到便宜存储上
clickhouse.com
。本项目初期数据量有限，但建议在需求文档中明确数据保留策略。例如，保留近一年的明细数据在线查询，将更早的数据转储归档（或汇总为K线后删除明细）。这一策略有助于控制硬盘占用。同时可配置定期Merge优化，防止小分块太多影响性能。

K线数据处理与多粒度支持

K线（OHLC）数据是交易分析和可视化的核心。原始行情（tick数据或秒级数据）需要聚合成不同周期的K线供策略和前端使用。当前系统已支持1分钟K线，但考虑到1分钟粒度可能对短线波动反应不够灵敏，我们计划支持更细粒度的K线，如5秒K线，甚至1秒K线，以满足高频分析需求。为此，开发需求做如下完善：

多周期K线的生成：典型做法是以固定的基础周期为单位累积生成长周期K线
piaohua.github.io
。例如，可选择1分钟作为基础K线周期，然后通过聚合1分钟K线得到5分钟、15分钟等更长周期的数据
piaohua.github.io
。但是在本项目中，我们需要比1分钟更细的5秒K线，这意味着基础数据粒度需更细。目前交易所WebSocket推送频率100ms，我们可以以1秒或5秒为最小周期直接生成K线。一种方案是：将实时数据按5秒窗口聚合生成5秒K线，然后再以5秒K线为基础进一步聚合1分钟K线（每1分钟 = 12个5秒K线）等
piaohua.github.io
。这样可以确保不同粒度间数据的一致性并减少重复计算。

K线数据存储：为提升查询效率，预计算并存储常用周期的K线数据是有必要的
clickhouse.com
。可以为每种周期建立单独的数据表，例如ethusdt_kline_5s, ethusdt_kline_1m, ethusdt_kline_15m等，字段包括时间、开盘价、最高价、最低价、收盘价、成交量等。每当基础数据（如tick或秒线）产生新记录，通过程序或数据库机制更新这些K线表。这种多张K线表的设计，可以在查询时直接获取指定周期的数据，而无需临时计算。虽然会占用一定存储，但由于K线数据相比tick数据压缩了大量信息，总量远小于明细数据，而且ClickHouse对聚合数据压缩存储效果很好。许多行情系统都会预先存储主要周期K线以加速前端图表加载和策略计算，这是较普遍的做法，并非反常。

聚合方法：为了自动、高效地维护K线，可以利用ClickHouse的物化视图 (Materialized View) 功能在数据插入时触发聚合。例如，针对1分钟K线，可以创建物化视图监听tick数据表，每收到数据就更新对应的1分钟OHLC聚合
clickhouse.com
clickhouse.com
。ClickHouse支持使用SUM/MIN/MAX/argMax/argMin等聚合函数计算OHLCV各项，并以AggregateFunction保存中间状态，然后写入目标K线表
clickhouse.com
clickhouse.com
。当用户查询时，再对聚合状态做Merge得到最终OHLC值
clickhouse.com
。以上技术可以应用于5秒、1分钟等不同周期：例如先建立5秒K线物化视图聚合tick数据，再建立1分钟K线物化视图聚合5秒K线表，以层层汇总实现多级周期
piaohua.github.io
piaohua.github.io
。物化视图将计算从查询时转移到插入时，保证查询各种周期K线时延迟极低
clickhouse.com
。如果实现上觉得物化视图复杂，也可以用独立的后端进程定时聚合更新K线表。无论如何，应避免每次前端请求都现算，预聚合能大幅提升响应速度。

粒度取舍：5秒K线能提供比1分钟更高的灵敏度，适合观察瞬时行情变化和策略短周期反应。然而，5秒K线在图表上刷新频率很高，人眼未必能轻易跟上，因此5秒K线更多是供策略模型内部使用或专业用户查看。一般交易前端常用的粒度还有15分钟、1小时、1日等较长周期，我们后续也可以通过聚合支持。这些不用全部存表，也可在查询时由基础周期汇总得到。需求上可以注明当前支持的K线周期清单，并允许根据股东需求增减。例如，初期支持5秒、1分钟、15分钟、1小时K线。如需新增其它周期，通过配置即可生成对应数据表或视图，保持系统设计的一致性和灵活性。

综上，建议创建多张K线表或视图来存储不同周期的数据，以换取查询和前端展示的高效。实践中这是常见折中做法，利用存储空间换时间，提高系统的响应速度和用户体验。考虑到ClickHouse对聚合数据压缩良好，增加的硬盘开销是可接受的，与带来的性能提升相比是值得的
clickhouse.com
。

策略回测与实盘交易模块

策略模块分为两个运行模式：历史回测和实盘交易。为了既保障开发测试的灵活性，又确保实盘运行的安全性，我们将在架构和流程上对两者进行区别对待，同时做到代码复用和结果一致。

策略回测：回测模块允许开发人员在历史数据上检验交易策略的效果。典型流程是从数据库中提取一段时间的历史行情（可选不同粒度K线或直接用tick数据），然后按时间推进模拟产生买卖信号、执行虚拟成交，记录每笔交易及资金曲线等结果。回测引擎用Python实现便于快速修改策略逻辑
blog.csdn.net
。开发者可以借助pandas等库进行指标计算，也可以使用专业回测框架（如Backtrader）提高效率
blog.csdn.net
。可视化：回测完成后，将结果（包括收益曲线、交易信号点等）保存并发送给前端，前端以图表形式展示，例如在K线图上标注买卖点，旁侧显示策略盈亏统计。这种图形化呈现让股东/合伙人能够直观理解策略表现，加深对策略的信心，也方便他们提出改进想法。

实盘交易：实盘模块在交易时间内持续运行，根据实时行情数据触发交易决策并下单到交易所。实盘程序需高度健壮，能应对各种异常（网络延迟、下单失败等）而不间断运行
blog.csdn.net
。我们将实盘交易逻辑用Java重写/实现（可参考回测的Python逻辑，但要经过严谨测试），部署为独立进程与数据采集、回测等隔离开
blog.csdn.net
。数据来源：交易引擎需要订阅最新市场数据以做出决策。可行方案有两种：1）直接从行情采集服务获取：例如行情服务通过内存队列将实时价格推送给交易引擎，使其几乎同时收到数据；2）引擎自行订阅行情：即交易引擎也连接交易所WebSocket获取所需品种的数据。方案1在架构上更清晰（行情统一由采集服务获取，然后分发），但需实现内部通信；方案2实现简单但会产生对交易所的重复订阅。鉴于目前币种不多，方案2可以接受，但我们倾向于方案1作为长远目标，以减少外部依赖和保证信号触发的一致性。
交易执行：当策略逻辑确定发出买卖信号后，交易引擎通过交易所提供的REST API执行下单
blog.csdn.net
。需要支持市价单、限价单等常用订单类型，并处理下单结果回报。遇到网络异常或接口错误要及时重试或报警，确保指令可靠送达
blog.csdn.net
。风控措施：交易引擎内置基本风险控制，如仓位管理和止损止盈。
blog.csdn.net
 在每次下单前检查账户持仓和资金，如果超出预设阈值则拒绝新开仓；同时为每笔交易设置止损价，当市价触及止损条件时发出平仓指令，防止亏损扩大
blog.csdn.net
。还可设定每日最大亏损、最大下单频率等规则，一旦触发则暂停策略并通知相关人员
blog.csdn.net
。这些风控规则在需求中需明确，让股东放心实盘风险可控。

隔离与测试：正如先前提到，实盘交易程序将独立运行，不与回测/数据服务共享进程。这样可以在开发人员调试回测或补数据时，不影响真实交易的进行，也防止测试代码误触实盘下单。这种隔离也是行业最佳实践之一
blog.csdn.net
。我们会提供一套模拟交易开关或模拟环境配置，让交易引擎可以连接到沙盒环境进行测试演练。当切换到真实交易所API时，需要双人核对和严格测试过策略逻辑，确保策略在实盘环境中的行为和在回测中的预期一致。这部分在需求中应该强调流程，例如“策略上线前需经过至少N天模拟盘验证，股东签字确认后方可连接真盘”之类，以保证稳健性。

前端设计与可视化交互

为了让不懂技术的股东和合伙人也能方便地理解和参与，本项目的前端将尽量可视化一切关键信息，并提供直观的交互方式。前端主要包括两个功能场景：历史回测展示和实盘行情/交易监控，二者将在统一的Web界面中呈现，用户可自由切换查看。

技术栈：前端采用 Vue3 框架，加上 Ant Design Vue 等UI组件库以构建友好的界面。图表部分使用成熟的金融行情图表库，例如 TradingView Charting Library、ECharts 或 Highcharts 等，来绘制蜡烛图、指标图等K线相关图表。这些库支持实时更新和丰富的标注，能够满足我们对回测和实时数据显示的要求。选择TradingView库还可以方便地使用其现有的技术指标插件，以后如股东提出新的分析指标需求，可快速配置呈现。

回测结果可视化：在回测模式下，用户界面将显示一段历史K线图，并在图上标记策略的买卖点（用箭头或圆点标识多空开平仓的位置）。旁边可以配以表格或文本，列出每笔交易的详细信息（时间、价格、盈亏等）和汇总绩效指标（如总收益率、最大回撤、胜率等）。通过图文结合，股东可以直观看到策略在历史上的表现，例如何时建仓、何时平仓、盈亏如何。如果股东有新的想法，例如“某段行情下策略是否错失机会”，开发人员可以根据这些可视化很快定位到对应时间段进行分析，沟通成本大大降低。

实时K线与交易监控：前端的实时模式将展现当前市场的实时K线图表（可选择5秒、1分钟等周期切换），随着行情更新而推送刷新，实现和交易所行情近乎同步的观看体验。K线图上同样可以叠加策略实时信号，例如当策略判定买入时，在当前K线上给出一个向上的箭头提示，即便实际下单也可一并展示（比如在图上标注“买单@价格”）。此外，还将有若干面板显示账户的实时状态：包括持仓和资金情况、当日盈亏、近期交易列表等。这样股东不仅能看K线，还能看到策略目前持有哪些币、浮动盈亏多少。一旦有警报（例如达到风控限制，或系统某模块异常），前端界面上会及时弹出通知或警示灯提示
blog.csdn.net
。

交互与参数调节：为了让股东的想法更容易传达，我们计划在前端提供一定的策略参数可调接口（视权限开放）。例如，如果策略包含几个主要参数（窗口长度、阈值等），可在前端设置面板里暴露调整滑块或输入框。股东修改参数后，可以一键运行新的回测，实时看到绩效变化。这一功能可以帮助非程序人员直接参与策略优化。当然，这需要后台支持动态运行回测，并实时将结果返回前端。初期可以只做简单演示级别参数调整，待稳定后再开放更多配置选项。

实现方式：前端通过调用后台API获取所需数据。回测数据在回测引擎跑完后会写入数据库或以JSON文件形式供前端查询。实盘行情则由后台行情服务通过WebSocket或Server-Sent Events推送到前端，以实现高频实时更新。我们将开发一个轻量的后台API层（可以集成在Java行情服务里使用Spring Boot提供REST/WS接口，也可以用Python Flask独立提供），专门为前端提供数据服务。例如：GET /api/kline?symbol=ethusdt&period=1m&start=...&end=... 获取历史K线，GET /api/backtest/result?id=123 获取某次回测结果集，WS /api/live/ethusdt 订阅实时行情数据流，等等。通过明确的API契约，前后端松耦合，方便日后分别迭代。

用户权限与安全：考虑到未来可能有多位股东/合伙人使用前端，我们会加入基本的权限管理。例如只有授权用户才能调参或启动回测，一般观察者账号仅能查看数据不可更改。交易下单则不会通过前端直接触发（防止误操作风险），而是始终由策略程序自动执行。但前端可以提供一个“紧急平仓”按钮，在需要人工干预时，通过调用后台交易引擎的接口来平掉所有持仓，以供紧急情况下股东手动干预风险。这些细节应在需求文档中提前声明，以达成共识。

总之，前端的目标是把复杂的数据和策略逻辑转化为直观的图表和控件，让非技术人员也能参与讨论和决策。
blog.csdn.net
提到可以借助Grafana等工具监控系统状态，在本项目中，我们将这些监控集成到定制前端中，集中展示策略表现和系统运行状态，避免股东在多个界面来回切换。

性能与扩展性考虑

虽然目前系统部署在单机且交易频率相对适中（100ms级别脉冲，无需微秒级响应），但我们仍需在需求阶段考虑未来的性能扩展，以保护项目的长期投资价值。

单机性能：在现有设计下，ClickHouse数据库能够支撑高频的数据写入和查询，内存和磁盘IO可能成为瓶颈但短期内问题不大。我们应明确硬件配置需求，如SSD磁盘、充足内存以发挥ClickHouse性能。当数据规模增长（比如tick数据过亿条），查询优化和表分区就重要。需求中可以规定，例如ethusdt表按日期分区，以加速按时间查询和管理历史分区。

多线程与异步：Java行情服务和交易引擎应采用多线程/异步方式处理网络IO和数据处理，避免因单线程阻塞导致的数据延迟。比如行情WS接收、解析、写库可以用独立线程，交易引擎也可用线程池处理多笔订单并发。由于Java天生支持并发，这部分只需在需求中强调“系统应支持并行处理行情与交易，保证100ms内完成业务逻辑”即可。

分布式部署：股东提到将来可能扩展为分布式云架构。如果预见到高灵敏度量化交易需求，我们需要保证架构容易横向扩展。需求上可注明：各模块应设计为无状态或轻状态服务，可按需增加实例部署在不同机器上。例如，行情采集服务可以部署多实例订阅不同交易对的数据，数据库可以搭建分布式集群以容纳更大数据量，前端服务也可部署CDN提高访问速度。未来如需上Docker或K8s容器编排，代码需做好配置化，日志和数据存储路径需可调整。这些并非当前必须实现，但在文档中写明可以让股东放心系统有升级空间。

故障恢复：需求应包含对于断线重连、数据遗漏、服务重启的处理策略。行情WS若断线，系统应自动重连并从断点继续（借助补齐程序保证数据完整）。交易引擎若宕机重启，应从持久化状态中恢复当前持仓和策略状态，避免重复下单或漏单。可以考虑在需求中增加“容错和恢复”章节，列举各模块发生故障时的预期行为。例如：“当行情采集进程崩溃时，系统应能够在N秒内由监控脚本拉起新进程，并从上次记录的时间点继续补录数据
medium.com
”。

安全和测试：性能之外，系统需要安全可靠。需求需强调对交易API的严格测试，尤其是真实环境下每一步都应验证。建议在上线前进行压力测试：模拟高频行情和密集交易，看系统最大吞吐量是否满足100ms节奏要求，并找出瓶颈加以优化。对于所有关键模块，编写单元测试和集成测试也是必要的。采用Python和Java混合开发时，要测试两边策略逻辑输出是否一致，以防范由于实现差异导致的策略偏差。

结论

综上所述，我们对原有开发需求进行了全面而深入的完善，明确了各模块的职责分工和技术选型，优化了数据库设计和数据处理流程，并加强了前端可视化和用户交互部分的要求。在该方案中：

后端以模块化架构运行，各组件各司其职：Python侧重快速开发与数据补齐、回测，Java负责高实时性的数据采集和交易执行，两者通过数据库和接口衔接
blog.csdn.net
quantstart.com
。

ClickHouse数据库设计精简且高效，利用列式存储和LowCardinality压缩优势，即使单表存多币种也能保持极小的冗余
clickhouse.com
。同时通过物化视图等机制预计算不同周期K线，换取查询与展示性能的提升
clickhouse.com
piaohua.github.io
。

系统状态跟踪和增量处理机制确保数据完整一致，每次运行都能从上次中断处继续，不重复处理也不遗漏数据
medium.com
。

前端功能丰富直观，既能用于策略历史回顾又能监控实盘运作，真正做到所见即所得，让非技术合伙人也能参与到策略迭代中来。

针对交易安全，提出了多项风控和隔离措施，并规划了充分的测试和容错方案，减少意外情况对资金的影响
blog.csdn.net
。

此开发需求文档旨在作为项目后续设计和实现的指南。在实际开发中，每一部分都会依据上述要求进行细化。通过上述周密的规划，我们有信心搭建一个高效、稳定、易扩展的量化交易系统雏形，为将来的功能增强和性能提升打下坚实基础。各位股东若有其他需求或想法，也可基于此文档提出，我们将评估后迭代更新，确保开发方案与大家的预期保持一致。